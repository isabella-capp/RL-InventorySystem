{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512fce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "\n",
    "# Our modules\n",
    "from src.agents.ppo_agent import PPOAgent\n",
    "from src.mdp import RewardFunction\n",
    "from src.agents.callbacks.learning_curve_callback import LearningCurveCallback\n",
    "from src.agents.metrics import TrainingMetrics, EvaluationMetrics\n",
    "from src.seeds import generate_seeds\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea598b85",
   "metadata": {},
   "source": [
    "# PPO Agent Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed788bc",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed50f7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ² PPO Training Seeds:\n",
      "   Training: 6163\n",
      "   Final evaluation: 1000 seeds starting at 6145\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "EPISODE_LENGTH = 1000  # days\n",
    "\n",
    "from src.seeds import TRAINING_SEED\n",
    "\n",
    "EVAL_SEEDS = generate_seeds(1000, start_index=0)\n",
    "\n",
    "metadata = {\n",
    "    \"k\": 30,\n",
    "    \"Q_max\": 30,\n",
    "    \"episode_length\": EPISODE_LENGTH,\n",
    "}\n",
    "\n",
    "print(\"ðŸŽ² PPO Training Seeds:\")\n",
    "print(f\"   Training: {TRAINING_SEED}\")\n",
    "print(f\"   Final evaluation: {len(EVAL_SEEDS)} seeds starting at {EVAL_SEEDS[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fca1820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Base environment: <function make_env.<locals>._init at 0x1256f6020>\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from src.environment.gym_env import InventoryEnvironment\n",
    "\n",
    "\n",
    "def make_env(\n",
    "    random_seed: int = TRAINING_SEED,\n",
    ") -> Any:\n",
    "    def _init() -> InventoryEnvironment:\n",
    "        return InventoryEnvironment(\n",
    "            k=metadata[\"k\"],\n",
    "            Q_max=metadata[\"Q_max\"],\n",
    "            episode_length=metadata[\"episode_length\"],\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "\n",
    "    return _init\n",
    "\n",
    "\n",
    "vec_env = DummyVecEnv([make_env()])\n",
    "\n",
    "vec_env = VecNormalize(\n",
    "    vec_env,\n",
    "    norm_obs=True,\n",
    "    norm_reward=True,\n",
    "    clip_obs=10.0,\n",
    "    clip_reward=50.0,\n",
    "    gamma=0.99,\n",
    ")\n",
    "\n",
    "print(f\"   Base environment: {make_env()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b48a55",
   "metadata": {},
   "source": [
    "## 3. Create and Train PPO Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802f1a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Agent created with seed 6163\n",
      "   Effective batch size: 128 samples\n"
     ]
    }
   ],
   "source": [
    "agent = PPOAgent(\n",
    "    env=vec_env,  # type: ignore\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    n_steps=2048,\n",
    "    batch_size=128,\n",
    "    n_epochs=10,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    gae_lambda=0.95,\n",
    "    tensorboard_log=\"./logs/ppo_agent\",  # Enable TensorBoard logging\n",
    "    policy_kwargs=dict(net_arch=[256, 256]),\n",
    "    seed=TRAINING_SEED,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "print(f\"PPO Agent created with seed {TRAINING_SEED}\")\n",
    "print(f\"   Effective batch size: {128} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb9cbe",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "Train the PPO agent with periodic evaluation.\n",
    "\n",
    "**Monitor with TensorBoard:**\n",
    "```bash\n",
    "tensorboard --logdir=./logs\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bcdc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be499876e8624a5281c11ce6c762780c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting training for 5,000,000 timesteps...\n",
      "   â‰ˆ 5,000 episodes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TOTAL_TIMESTEPS = 5_000_000\n",
    "\n",
    "print(f\"ðŸš€ Starting training for {TOTAL_TIMESTEPS:,} timesteps...\")\n",
    "print(f\"   â‰ˆ {TOTAL_TIMESTEPS // EPISODE_LENGTH:,} episodes\\n\")\n",
    "\n",
    "# Initialize plot classes\n",
    "training_plots = TrainingMetrics()\n",
    "evaluation_plots = EvaluationMetrics()\n",
    "\n",
    "# Callbacks\n",
    "learning_curve_callback = LearningCurveCallback()\n",
    "\n",
    "agent.train(\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    progress_bar=True,\n",
    "    callbacks=learning_curve_callback,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"  Episodes recorded: {len(learning_curve_callback.episode_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843bc61",
   "metadata": {},
   "source": [
    "## 6. Save the Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(metadata=metadata)\n",
    "\n",
    "# Save VecNormalize statistics\n",
    "vec_env.save(\"./models/vec_normalize_ppo.pkl\")  # type: ignore\n",
    "print(\"âœ… Model and VecNormalize statistics saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f084f",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Training Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ae35ff",
   "metadata": {},
   "source": [
    "### 1. Learning Curve (Training Stability)\n",
    "Average Reward per Episode vs. Timesteps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = int(0.1 * (TOTAL_TIMESTEPS // EPISODE_LENGTH))\n",
    "\n",
    "# Learning Curve: Use data from callback\n",
    "training_plots.plot_learning_curve(\n",
    "    episode_timesteps=learning_curve_callback.episode_timesteps,\n",
    "    episode_rewards=learning_curve_callback.episode_rewards,\n",
    "    window=max(window, 5),\n",
    "    title=\"Learning Curve: PPO Agent Training Progress\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf3bde",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 2: Evaluation & Testing\n",
    "Run N=100 independent test episodes with deterministic policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TEST_EPISODES = len(EVAL_SEEDS)\n",
    "\n",
    "reward_fn = RewardFunction()\n",
    "test_episodes_ppo = []\n",
    "\n",
    "print(f\"ðŸ§ª Running {N_TEST_EPISODES} test episodes with deterministic seeds...\")\n",
    "\n",
    "for ep, seed in tqdm(\n",
    "    enumerate(EVAL_SEEDS), total=N_TEST_EPISODES, desc=\"Evaluating PPO\"\n",
    "):\n",
    "    eval_env = DummyVecEnv([make_env(random_seed=seed)])\n",
    "\n",
    "    eval_env = VecNormalize.load(\"./models/vec_normalize_ppo.pkl\", eval_env)\n",
    "    eval_env.training = False\n",
    "    eval_env.norm_reward = False\n",
    "\n",
    "    obs = eval_env.reset()\n",
    "\n",
    "    ppo_data = {\n",
    "        \"net_inv_0\": [],\n",
    "        \"net_inv_1\": [],\n",
    "        \"q0\": [],\n",
    "        \"q1\": [],\n",
    "        \"demand_0\": [],\n",
    "        \"demand_1\": [],  # Daily demand per product\n",
    "        \"ordering_cost\": [],\n",
    "        \"holding_cost\": [],\n",
    "        \"shortage_cost\": [],\n",
    "        \"total_daily_cost\": [],\n",
    "    }\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Agent gets normalized observations (same as training)\n",
    "        action, _ = agent.model.predict(obs, deterministic=True)  # type: ignore\n",
    "\n",
    "        # Step the vectorized environment\n",
    "        obs, reward, done_vec, info = eval_env.step(action)\n",
    "        done = done_vec[0]\n",
    "\n",
    "        # Get the underlying environment for logging (before normalization)\n",
    "        raw_env = eval_env.venv.envs[0]  # type: ignore\n",
    "        action_obj = raw_env.action_space_config.get_action(action[0])\n",
    "        raw_info = info[0]\n",
    "\n",
    "        # Log Inventory & Actions\n",
    "        ppo_data[\"net_inv_0\"].append(raw_info[\"net_inventory\"][0])\n",
    "        ppo_data[\"net_inv_1\"].append(raw_info[\"net_inventory\"][1])\n",
    "        ppo_data[\"q0\"].append(action_obj.order_quantities[0])\n",
    "        ppo_data[\"q1\"].append(action_obj.order_quantities[1])\n",
    "\n",
    "        # Log Daily Demand per product\n",
    "        ppo_data[\"demand_0\"].append(raw_info[\"total_demand\"][0])\n",
    "        ppo_data[\"demand_1\"].append(raw_info[\"total_demand\"][1])\n",
    "\n",
    "        # Compute Daily Cost Components using RewardFunction\n",
    "        state = raw_env.get_current_state()\n",
    "        costs = reward_fn.calculate_costs(state, action_obj)\n",
    "\n",
    "        # Append to lists\n",
    "        ppo_data[\"ordering_cost\"].append(costs.ordering_cost)\n",
    "        ppo_data[\"holding_cost\"].append(costs.holding_cost)\n",
    "        ppo_data[\"shortage_cost\"].append(costs.shortage_cost)\n",
    "        ppo_data[\"total_daily_cost\"].append(costs.total_cost)\n",
    "\n",
    "    test_episodes_ppo.append(ppo_data)\n",
    "    eval_env.close()\n",
    "\n",
    "print(\n",
    "    f\"âœ… Collected {N_TEST_EPISODES} test episodes for PPO with normalized observations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194d067",
   "metadata": {},
   "source": [
    "### Warm-up Period Analysis (Welch's Graphical Procedure)\n",
    "Since the simulation starts with initial inventory conditions, the early data may be biased (transient phase).   \n",
    "We use Welch's procedure to identify when steady-state begins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c833d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welch's Graphical Procedure for Warm-up Detection\n",
    "n_days, n_reps, WARMUP_LENGTH = evaluation_plots.plot_welch_procedure(\n",
    "    test_episodes=test_episodes_ppo,\n",
    "    window_size=25,\n",
    "    title=\"Warm-up Period Analysis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4756cde9",
   "metadata": {},
   "source": [
    "## Cost Component Breakdown (Economic Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print evaluation statistics\n",
    "stats = evaluation_plots.print_evaluation_statistics(\n",
    "    test_episodes=test_episodes_ppo,\n",
    "    warmup_length=WARMUP_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9ed92",
   "metadata": {},
   "source": [
    "### Daily Cost Evolution\n",
    "\n",
    "Aggregated daily cost statistics across all test episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c5f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Cost Analysis\n",
    "evaluation_plots.plot_daily_cost_analysis(\n",
    "    test_episodes=test_episodes_ppo,\n",
    "    warmup_length=WARMUP_LENGTH,\n",
    "    title=\"Daily Cost Evolution - PPO Agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858993ca",
   "metadata": {},
   "source": [
    "Grouped bar chart decomposed into Ordering, Holding, and Shortage costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e8a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Component Breakdown by Product\n",
    "evaluation_plots.plot_cost_breakdown_by_product(\n",
    "    test_episodes=test_episodes_ppo,\n",
    "    warmup_length=WARMUP_LENGTH,\n",
    "    n_days=n_days,\n",
    "    title=\"Cost Component Breakdown by Product\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c2511",
   "metadata": {},
   "source": [
    "### 4. Operational Time Series (Behavioral Analysis)\n",
    "Snapshot of inventory levels, orders, and demand over time for selected episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eab641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select representative episode based on STEADY-STATE costs (excluding warmup)\n",
    "episode_ss_costs = [\n",
    "    np.mean(ep[\"total_daily_cost\"][WARMUP_LENGTH:]) for ep in test_episodes_ppo\n",
    "]\n",
    "global_ss_mean = np.mean(episode_ss_costs)\n",
    "\n",
    "# Find episode closest to mean\n",
    "representative_idx = np.argmin(np.abs(np.array(episode_ss_costs) - global_ss_mean))\n",
    "\n",
    "# Also find best and worst episodes\n",
    "best_idx = np.argmin(episode_ss_costs)\n",
    "worst_idx = np.argmax(episode_ss_costs)\n",
    "\n",
    "print(f\"ðŸ“Š Episode Selection (Based on Steady-State Costs):\")\n",
    "print(\n",
    "    f\"   Representative: Episode {representative_idx} (cost: ${episode_ss_costs[representative_idx]:.2f})\"\n",
    ")\n",
    "print(\n",
    "    f\"   Best:           Episode {best_idx} (cost: ${episode_ss_costs[best_idx]:.2f})\"\n",
    ")\n",
    "print(\n",
    "    f\"   Worst:          Episode {worst_idx} (cost: ${episode_ss_costs[worst_idx]:.2f})\"\n",
    ")\n",
    "print(f\"   Mean across all episodes: ${global_ss_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“ˆ Plotting Representative Episode (Steady-State Period)...\")\n",
    "evaluation_plots.plot_operational_timeseries(\n",
    "    episode_data=test_episodes_ppo[representative_idx],\n",
    "    title=\"Operational Time Series - PPO Agent (Representative Episode)\",\n",
    "    start_day=WARMUP_LENGTH,\n",
    "    max_days=200,  # Show only 200 days for clarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf2314",
   "metadata": {},
   "source": [
    "### 5. Inventory Distribution Histogram (Risk Profile)\n",
    "Distribution of Net Inventory levels over all test episodes. Red = Backlog (I < 0), Green = On-Hand (I > 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c52585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inventory Distribution Histogram (Steady-State only)\n",
    "evaluation_plots.plot_inventory_histogram(\n",
    "    test_episodes=test_episodes_ppo,\n",
    "    warmup_length=WARMUP_LENGTH,\n",
    "    title=\"Inventory Distribution - Risk Profile (PPO Agent, Steady-State)\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-inventorysystem (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
